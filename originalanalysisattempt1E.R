##if packages are not already installed in R then install, installation shown in comments
## reference for scde error modelling: https://hms-dbmi.github.io/scde/diffexp.html


#devtools::install_version('flexmix', '2.3-13') this version of flexmix is needed to be downloaded before SCDE version 1.99.1
#devtools::install_github('hms-dbmi/scde', build_vignettes = FALSE)
library(scde)
library(parallel)
#install_version('mclust', version = '5.4.10', repos = 'http://lib.stat.cmu.edu/R/CRAN/')
library(mclust)
##install.packages("Rtsne")
library(Rtsne)
##install.packages("igraph")
library(igraph)
##install.packages("FactoMineR")
library(FactoMineR)
##install.packages("parallelly")
##library(parallelly)


data <- read.table('GSE67835_CPM_named_clean.tsv', header = TRUE, row.names = 1, sep ='\t')
###Convert all non gene columns to numeric
data[,1:466] <- sapply(data[,1:466], as.integer)

##clean.counts function from the scde package to remove low-count genes and low-quality cells
clean_data <- clean.counts(data, min.lib.size=1000, min.reads = 1, min.detected = 1)

##fit error model to each cell
##  fit an error model to each cell and filter out cells 

model <- scde.error.models(counts = clean_data, n.cores = 4, threshold.segmentation = TRUE, save.crossfit.plots = FALSE, save.model.plots = FALSE, verbose = 1)
# filter out cells that don't show positive correlation with
# the expected expression magnitudes (very poor fits)
valid.cells <- model$corr.a > 0 ##only want these
table(valid.cells) ##should print out 466 into terminal

model <- model[valid.cells, ]
write.csv(as.data.frame(model), 'model.csv')  ##creating a file to have a copy in case any crashing occurs

p.self.fail <- scde.failure.probability(models = model, counts = clean_data)
n.simulations <- 500; k <- 0.9;
cell.names <- colnames(clean_data); names(cell.names) <- cell.names;
dl <- mclapply(1:n.simulations,function(i) {
  scd1 <- do.call(cbind,lapply(cell.names,function(nam) {
    x <- clean_data[,nam];
    # replace predicted drop outs with NAs
    x[!as.logical(rbinom(length(x),1,1-p.self.fail[,nam]*k))] <- NA;
    x;
  }))
  rownames(scd1) <- rownames(clean_data); 
  # calculate correlation on the complete observation pairs
  cor(log10(scd1+1),use="pairwise.complete.obs");
}, mc.cores = 4)
# mclapply() used to parallelize the computation across cores
##pairwisedistancematrix generated
# calculate average distance across sampling rounds
direct.dist <- as.dist(1-Reduce("+",dl)/length(dl))
distances <- as.data.frame(as.matrix(direct.dist))
write.csv(as.data.frame(distances), 'distancematrix.csv')
distancematrix <- as.matrix(direct.dist)

##data needs to be normalised for clustering
## distance matrix is normalized and used for dimensionality reduction with t-SNE (t-Distributed Stochastic Neighbor Embedding)
normalised_data <- normalize_input(distancematrix)
## Dimensionality reduction using t-SNE:
# setting theta to 0.0 indicates that the algorithm should be run with the exact optimization method
## perplexity measures the  number of neighbors that each point has in the high-dimensional space
tsne_results <- Rtsne(normalised_data, perplexity = 10, theta = 0.0)  
# output of the t-SNE algorithm 
# the Y matrix contains the lower-dimensional coordinates generated by the t-SNE algorithm - format may not be easy to work with 
tsnedf <- as.data.frame(tsne_results$Y) ##converting the Y matrix into a data frame, which can be more easily manipulated and plotted


##CLUSTERING WITH MCLUST

clustered_data <- Mclust (tsnedf) ## clustering analysis, storing in variable clustered_data

## visualisation of clustering analysis
par(mfrow = c(1, 2))
plot(clustered_data, what = 'BIC', xlab = 'Number of Components')
plot(clustered_data, what = 'classification')

# Set up the PNG file, can alter height and width if need be
png("mclust_plot.png", width = 800, height = 600)
## plot results 
par(mfrow = c(1, 2))
# generating a plot of the Bayesian information criterion (BIC) values
## what = 'BIC', specifies that the BIC values should be plotted as done in the paper
## xlab = specifies the label for the x-axis of the plot.
plot(clustered_data, what = 'BIC', xlab = 'Number of Components')
#  plot where each data point is colored according to its assigned class
plot(clustered_data, what = 'classification')
## what = "BIC" argument tells plot to display the BIC values, which are a measure of model fit for the clustering model, for different numbers of clusters.
## This creates a plot with the BIC value on the left and the classification results on the right.
# Close the PNG file and save
dev.off()

